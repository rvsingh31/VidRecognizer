{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "from InceptionTF import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder for storing the frames\n",
    " \n",
    "splitfiledir = r\"..\\ucfTrainTestlist\"\n",
    "splitfile = \"mytest.txt\"\n",
    "splitname = splitfile.split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeFramesAndFlows(framesdir,splitfiledir,splitfile,splitname):\n",
    "    \n",
    "    #Read splitfile\n",
    "    lines = open(os.path.join(splitfiledir,splitfile),\"r\")\n",
    "    for line in lines:\n",
    "        arr = line.split(\" \")\n",
    "        vidclass = arr[1] \n",
    "        line = arr[0].split(\"/\")\n",
    "        action = line[0]\n",
    "        filename = line[1]\n",
    "        actionpath = os.path.join(framesdir,splitname,action)\n",
    "        framepath = os.path.join(actionpath,filename,\"frames\")\n",
    "        flowpath = os.path.join(actionpath,filename,\"flows\")\n",
    "        \n",
    "        #Create folder for Action\n",
    "        if not os.path.exists(actionpath):\n",
    "            os.mkdir(actionpath)\n",
    "        \n",
    "        #Create folder for videofile\n",
    "        if not os.path.exists(os.path.join(actionpath,filename)):\n",
    "            os.mkdir(os.path.join(actionpath,filename))\n",
    "        \n",
    "        #Create folder for frames\n",
    "        if not os.path.exists(framepath):\n",
    "            os.mkdir(framepath)\n",
    "            \n",
    "        #Create folder for flows\n",
    "        if not os.path.exists(flowpath):\n",
    "            os.mkdir(flowpath)\n",
    "\n",
    "        #Read video and collect frames, flows\n",
    "        vidcap = cv2.VideoCapture(os.path.join(path,action,filename))\n",
    "        count = 0 \n",
    "        prevFrame = None\n",
    "        nextFrame = None\n",
    "        while True:\n",
    "            success,image = vidcap.read()\n",
    "            #Resize image to remain consistent with BN Inception model\n",
    "            if not success:\n",
    "                break\n",
    "            image = cv2.resize(image,(224,224))\n",
    "            frame = \"frame_%d.jpg\"%count\n",
    "            flow_x = \"flow_x_%d.jpg\"%count\n",
    "            flow_y = \"flow_y_%d.jpg\"%count\n",
    "            framename = os.path.join(framepath,frame)\n",
    "            flowname_x = os.path.join(flowpath,flow_x)\n",
    "            flowname_y = os.path.join(flowpath,flow_y)\n",
    "            if count == 0:\n",
    "                prevFrame = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "                cv2.imwrite(framename,image)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                count += 1\n",
    "                continue\n",
    "            \n",
    "            nextFrame = cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "            cv2.imwrite(framename,image)\n",
    "            optical_flow = cv2.optflow.DualTVL1OpticalFlow_create()\n",
    "            flow = optical_flow.calc(prevFrame, nextFrame, None)\n",
    "            prevFrame = nextFrame\n",
    "            flow[...,0] = cv2.normalize(flow[...,0],None,0,255,cv2.NORM_MINMAX)\n",
    "            flow[...,1] = cv2.normalize(flow[...,1],None,0,255,cv2.NORM_MINMAX)\n",
    "            cv2.imwrite(flowname_x,flow[...,0])\n",
    "            cv2.imwrite(flowname_y,flow[...,1])\n",
    "\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            count += 1\n",
    "            \n",
    "        filename = os.path.join(actionpath,filename,\"info.txt\")\n",
    "        #Store the frames count in txt file\n",
    "        rate = open(filename,\"w\")\n",
    "        rate.write(\"frames:\"+str(count))\n",
    "        rate.write(\"\\n\")\n",
    "        rate.write(\"class:\"+vidclass)\n",
    "        rate.close()\n",
    "\n",
    "        #Close the video object\n",
    "        vidcap.release()\n",
    "\n",
    "    print (\"Stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames stored already!\n"
     ]
    }
   ],
   "source": [
    "#Store Frames\n",
    "framesdir = r\"..\\FramesFlows\"\n",
    "\n",
    "path = r\"E:\\capstone_adbi_data\\UCF-101\"\n",
    "\n",
    "\n",
    "if not os.path.exists(os.path.join(framesdir,splitname)):\n",
    "    os.mkdir(os.path.join(framesdir,splitname))\n",
    "    storeFramesAndFlows(framesdir,splitfiledir,splitfile,splitname)\n",
    "else:\n",
    "    print (\"Frames stored already!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFF(classpath):\n",
    "    frames = list()\n",
    "    xflows = list()\n",
    "    yflows = list()\n",
    "    #Stack up frames and flows\n",
    "    min_frames = float('inf')\n",
    "    min_flows = float('inf')\n",
    "    y_true = None\n",
    "    for folder in os.listdir(classpath):\n",
    "        folderpath = os.path.join(classpath,folder)\n",
    "        # infopath = folderpath\\info.txt\n",
    "        infopath = os.path.join(folderpath,\"info.txt\")\n",
    "        # Read info.txt for extracting class\n",
    "        f = open(infopath,\"r\")\n",
    "        y_true = int(f.readlines()[1].strip().split(':')[1])\n",
    "\n",
    "        # Collect frames\n",
    "        imgpath = os.path.join(folderpath,\"frames\")\n",
    "        flowspath = os.path.join(folderpath,\"flows\")\n",
    "\n",
    "        allframes = os.listdir(imgpath)\n",
    "        allflows = os.listdir(flowspath)\n",
    "        min_frames = min(len(allframes),min_frames)\n",
    "\n",
    "        #Sort frames sequentially\n",
    "        allframes.sort(key = lambda x: int(x.split('_')[1].split('.')[0]))\n",
    "\n",
    "        #Sort flows sequentially\n",
    "        allxflows = list(filter(lambda k: k.split('_')[1] == 'x',allflows))\n",
    "        allyflows = list(filter(lambda k: k.split('_')[1] == 'y',allflows))\n",
    "        allxflows.sort(key = lambda x: int(x.split('_')[2].split('.')[0]))\n",
    "        allyflows.sort(key = lambda x: int(x.split('_')[2].split('.')[0]))\n",
    "        min_flows = min(len(allxflows),min_flows)\n",
    "\n",
    "        stack = list()\n",
    "        for frame in allframes:\n",
    "            img = cv2.imread(os.path.join(imgpath,frame))\n",
    "            grayimg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "            grayimg = np.expand_dims(grayimg,axis = 2)\n",
    "            stack.append(grayimg)\n",
    "        frames.append(np.array(stack))\n",
    "        \n",
    "        \n",
    "        #Collect flows\n",
    "        xstack = list()\n",
    "        ystack = list()\n",
    "        for xflow,yflow in zip(allxflows,allyflows):\n",
    "            xfl = cv2.imread(os.path.join(flowspath,xflow))\n",
    "            yfl = cv2.imread(os.path.join(flowspath,yflow))\n",
    "            grayxfl = cv2.cvtColor(xfl,cv2.COLOR_BGR2GRAY)\n",
    "            grayyfl = cv2.cvtColor(yfl,cv2.COLOR_BGR2GRAY)\n",
    "            grayxfl = np.expand_dims(grayxfl,axis = 2)\n",
    "            grayyfl = np.expand_dims(grayyfl,axis = 2)\n",
    "            xstack.append(grayxfl)\n",
    "            ystack.append(grayyfl)\n",
    "        xflows.append(np.array(xstack))\n",
    "        yflows.append(np.array(ystack))\n",
    "    \n",
    "    return frames, xflows, yflows, y_true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpath = r\"../FramesFlows/mytest\"\n",
    "frames_dict = dict()\n",
    "xflows_dict = dict()\n",
    "yflows_dict = dict()\n",
    "for each in os.listdir(dfpath):\n",
    "    classpath = os.path.join(dfpath,each)\n",
    "    frames, xflows, yflows, class_name = getFF(classpath)\n",
    "    frames_dict[class_name] = frames\n",
    "    xflows_dict[class_name] = xflows\n",
    "    yflows_dict[class_name] = yflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 224, 224, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dictionary key is the class\n",
    "#Second index represents video number in that class (starting from 0)\n",
    "#Shape shows NO_OF_FRAMES*HEIGHT_OF_IMAGE*WIDTH_OF_IMAGE\n",
    "frames_dict[2][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decided to keep number of segments as 3 for now (consistent with the paper)\n",
    "segments = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data, classes = 101):\n",
    "    \"\"\"\n",
    "    :param data: data to be one hot encoded\n",
    "    :return: np array with one hot encoding\n",
    "    \"\"\"\n",
    "    labels = np.zeros((data.size, classes))\n",
    "    labels[np.arange(data.size), data - 1] = 1\n",
    "    return labels\n",
    "\n",
    "def getSegments(frames_dict,segments=3):\n",
    "    #Get k(equal to segments) random samples(snippets) from frames\n",
    "    framesSegments = defaultdict(list)\n",
    "    ySegments = defaultdict(list)\n",
    "    np.random.seed(0)\n",
    "    for class_name in frames_dict.keys():\n",
    "        videos = frames_dict[class_name]\n",
    "        for video in videos:\n",
    "            vsegs = np.array_split(video,segments)\n",
    "            for x in range(len(vsegs)):\n",
    "                idx = np.random.randint(vsegs[x].shape[0])\n",
    "                #Append random snippet to segments dictionary\n",
    "                framesSegments[x].append(vsegs[x][idx])\n",
    "                ySegments[x].append(class_name)\n",
    "    \n",
    "    #One hot encoding\n",
    "    for x in range(segments):\n",
    "        ySegments[x] = one_hot_encode(np.array(ySegments[x]))\n",
    "\n",
    "    return framesSegments, ySegments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "framesSegments, yFramesTrueSegments = getSegments(frames_dict)\n",
    "xFlowsSegments, yFlowsTrueSegments = getSegments(xflows_dict)\n",
    "yFlowsSegments, yFlowsTrueSegments = getSegments(yflows_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#key is segment number(0,1...segments) and second index in the image\n",
    "framesSegments[0][1].shape\n",
    "#key is segment number(0,1,...segments) and second index returns one hot encoded class for the respective image\n",
    "yFramesTrueSegments[2][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need separate networks (equal to segments) working in parallel \n",
    "#Parameters for BN-Inception network\n",
    "#We need Inputs: a tensor of shape [batch_size, height, width, channels].\n",
    "imageHeight = 240\n",
    "imageWidth = 240\n",
    "colorChannels = 3\n",
    "num_classes = 101\n",
    "learning_rate = 0.001\n",
    "X = tf.placeholder(tf.float32,(None,imageHeight,imageWidth,colorChannels))\n",
    "y = tf.placeholder(tf.float32,(None,num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLogits(X,learning_rate,imageHeight, imageWidth, colorChannels, num_classes):\n",
    "    logits, end_points = inception_v2(inputs = X,num_classes = num_classes,reuse = True)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits = getLogits(X, learning_rate,imageHeight, imageWidth, colorChannels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
